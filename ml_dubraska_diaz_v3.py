# -*- coding: utf-8 -*-
"""ML_DUBRASKA_DIAZ_V3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pk8glyrTxxJXb6Oux0tW0xc-pGcbm4cS

Importar set de datos
"""

#Evaluación DIP266 y DIP267
import pandas as pd

url='https://drive.google.com/file/d/1SyIPpfP7ddGWr_XCENqyOWzS6Y9q1j4f/view?usp=sharing'
file_id=url.split('/')[-2]
dwn_url='https://drive.google.com/uc?id=' + file_id
credit=CC_GENERAL_CUST_ID  = pd.read_csv(dwn_url)
print(credit.head())

"""Librerías"""

# Commented out IPython magic to ensure Python compatibility.
#cargo todas las librerias posibles que creo que puedo utilizar
import numpy as np
import seaborn as sns

import matplotlib.pyplot as plt
from sklearn import preprocessing as pp
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from datetime import datetime
from datetime import timedelta
import statsmodels.formula.api as sn
# %matplotlib inline
import scipy.stats as stats
from datetime import datetime
import statsmodels.api as sm
from sklearn import metrics
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVC, LinearSVC
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)
np.set_printoptions(precision=3)
sns.set(style="darkgrid")
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12
#CC_GENERAL_CUST_ID = pd.read_csv('/CC GENERAL.csv')
#CC_GENERAL=CC_GENERAL_CUST_ID.drop(['CUST_ID'], axis = 1)
credit.describe()

"""#Explorando la data

"""

credit.isna().sum()

credit.isnull().any()

# Existen valores faltantes por tanto se leminaran con la mediana

credit['CREDIT_LIMIT'].fillna(credit['CREDIT_LIMIT'].median(),inplace=True)

credit['CREDIT_LIMIT'].count()


credit['MINIMUM_PAYMENTS'].median()
credit['MINIMUM_PAYMENTS'].fillna(credit['MINIMUM_PAYMENTS'].median(),inplace=True)

# Now again check the missing values.

credit.isnull().any()

"""Revisando relación de Variables

AVANCE DE EFECTIVO vs. LÍMITE DE CRÉDITO
"""

X=credit['CASH_ADVANCE_TRX']
Y=credit['CREDIT_LIMIT']

plt.figure(figsize=(5,5))
ax = plt.axes()
ax.set_facecolor("NONE")
plt.title("T Avance Efectivo vs Límite de Crédito")
plt.xlabel('Transacciones Avance Efectivo')
plt.ylabel('Límite Crédito')
plt.scatter(X,Y)
plt.show()

"""MONTO de PAGOS vs MONTO de COMPRAS

"""

X=credit['PAYMENTS']
Y=credit['PURCHASES']
plt.figure(figsize=(5,5))
ax = plt.axes()
ax.set_facecolor("NONE")
plt.title("Pagos vs Compras")
plt.xlabel('Pagos')
plt.ylabel('Compras')
plt.scatter(X,Y)
plt.show()

# Histogramas
credit_HIST=credit.drop(['CUST_ID'], axis = 1)
fig, axes = plt.subplots(len(credit_HIST.columns)//4, 4, figsize=(24, 24))
i = 0
colors = ['tab:blue', 'tab:olive', 'tab:green', 'tab:pink',
          'tab:blue', 'tab:olive', 'tab:green', 'tab:pink',
          'tab:blue', 'tab:olive', 'tab:green', 'tab:pink',
          'tab:blue', 'tab:olive', 'tab:green', 'tab:pink']
for triaxis in axes:
    for axis in triaxis:
      credit_HIST.hist(column = credit_HIST.columns[i], bins = 15, ax=axis,color=colors[i])
      i = i+1

"""1. Promedio mensual de compra y avance de efectivo

Avance de Efectivo Mensual
"""

credit['Monthly_cash_advance']=credit['CASH_ADVANCE']/credit['TENURE']
credit[credit['ONEOFF_PURCHASES']==0]['ONEOFF_PURCHASES'].count()

"""Compra Promedio Mensual"""

credit['Monthly_avg_purchase']=credit['PURCHASES']/credit['TENURE']
print(credit['Monthly_avg_purchase'].head(),'\n ',
credit['TENURE'].head(),'\n', credit['PURCHASES'].head())

"""Compras según pago : por cuotas/ sin cuotas"""

credit.loc[:,['ONEOFF_PURCHASES','INSTALLMENTS_PURCHASES']]

"""Estudiando el comportamiento de compra """

def purchase(credit):
    if (credit['ONEOFF_PURCHASES']==0) & (credit['INSTALLMENTS_PURCHASES']==0):
        return 'Sin Compras'
    if (credit['ONEOFF_PURCHASES']>0) & (credit['INSTALLMENTS_PURCHASES']>0):
         return 'Ambas Modalidades'
    if (credit['ONEOFF_PURCHASES']>0) & (credit['INSTALLMENTS_PURCHASES']==0):
        return 'Pago Único'
    if (credit['ONEOFF_PURCHASES']==0) & (credit['INSTALLMENTS_PURCHASES']>0):
        return 'Compra por cuotas'
credit['purchase_type']=credit.apply(purchase,axis=1)

credit['purchase_type'].value_counts()

"""Saldo de la tarjeta vs. Limite de crédito"""

credit['limit_usage']=credit.apply(lambda x: x['BALANCE']/x['CREDIT_LIMIT'], axis=1)
credit['limit_usage'].head()

"""Explorando el comportamiento del pago mínimo"""

credit['payment_minpay']=credit.apply(lambda x:x['PAYMENTS']/x['MINIMUM_PAYMENTS'],axis=1)
credit['payment_minpay'].describe()

"""Aún logaritmizando la data la desviación estándar es alta, lo que me motiva a eliminar los valores extremos"""

# log tranformation
cr_log=credit.drop(['CUST_ID','purchase_type'],axis=1).applymap(lambda x: np.log(x+1))

col=['BALANCE','PURCHASES','CASH_ADVANCE','TENURE','PAYMENTS','MINIMUM_PAYMENTS','PRC_FULL_PAYMENT','CREDIT_LIMIT']
cr_pre=cr_log[[x for x in cr_log.columns if x not in col ]]
cr_pre.columns

cr_log.columns

"""Estableciendo relaciones entre variables"""

x=credit.groupby('purchase_type').apply(lambda x: np.mean(x['payment_minpay']))
type(x)
x.values

fig,Tenencia=plt.subplots()
Tenencia.barh(y=range(len(x)), width=x.values,align='center',color='skyblue')
Tenencia.set(yticks= np.arange(len(x)),yticklabels = x.index);
Tenencia.set_facecolor('none')
plt.title('Tenencia de TDC en meses por tipo de compra')
plt.xlabel('Tenecia en meses')
plt.ylabel('Compras por opción de pago')

w=credit.groupby('purchase_type').apply(lambda x: np.mean(x['Monthly_cash_advance']))
fig,Avance_efectivo_mensual=plt.subplots()
Avance_efectivo_mensual.barh(y=range(len(w)), width=w.values,align='center',color='skyblue')
Avance_efectivo_mensual.set(yticks= np.arange(len(w)),yticklabels = x.index);
Avance_efectivo_mensual.set_facecolor('none')
plt.title('Avance efectivo vs. Tipo de Compra')
plt.xlabel('Avance efectivo promedio TDC')
plt.ylabel('Tipo de Compra')

z=credit.groupby('purchase_type').apply(lambda x: np.mean(x['limit_usage']))
fig,Limiteuso=plt.subplots()
Limiteuso.barh(y=range(len(z)), width=z.values,align='center',color='skyblue')
Limiteuso.set(yticks= np.arange(len(z)),yticklabels = x.index);
Limiteuso.set_facecolor('none')
plt.title('Límite de Uso vs. Tipo de Compra')
plt.xlabel('Límite de Uso de la TDC')
plt.ylabel('Tipo de Compra')

"""Buscando eliminar colinealidad en las variables"""

cre_original=pd.concat([credit,pd.get_dummies(credit['purchase_type'])],axis=1)

# creating Dummies for categorical variable
cr_pre['purchase_type']=credit.loc[:,'purchase_type']
pd.get_dummies(cr_pre['purchase_type'])

cr_dummy=pd.concat([cr_pre,pd.get_dummies(cr_pre['purchase_type'])],axis=1)
l=['purchase_type']
cr_dummy=cr_dummy.drop(l,axis=1)
cr_dummy.isnull().any()

"""Observando correlación con la data dicotomizada"""

sns.heatmap(cr_dummy.corr(),
              cmap="YlGnBu"
            )

"""Buscando Correlación con la data original"""

sns.heatmap(credit.corr(),
              xticklabels=credit.columns,
              yticklabels=credit.columns,cmap="YlGnBu"
            )

"""No se observan diferencias en las correlaciones al dicotomizar la data, aún no es comparable, aún no hay garantía de no multicolinealidad, las gráficas no agregan nada al análisis final y ocupan mucho espacio, no las incluí en el documento."""

from sklearn.preprocessing import  StandardScaler
sc=StandardScaler()
cr_scaled=sc.fit_transform(cr_dummy)
cr_scaled

"""Reducción de componentes de datos"""

from sklearn.decomposition import PCA

#17 variables.
pc=PCA(n_components=17)
cr_pca=pc.fit(cr_scaled)
#trataremos de reducir las variables de forma eficiente
sum(cr_pca.explained_variance_ratio_)

var_ratio={}
for n in range(2,18):
    pc=PCA(n_components=n)
    cr_pca=pc.fit(cr_scaled)
    var_ratio[n]=sum(cr_pca.explained_variance_ratio_)

var_ratio

pc=PCA(n_components=10)
p=pc.fit(cr_scaled)
cr_scaled.shape

p.explained_variance_

np.sum(p.explained_variance_)

var_ratio

pd.Series(var_ratio).plot()

pc_final=PCA(n_components=10).fit(cr_scaled)

reduced_cr=pc_final.fit_transform(cr_scaled)
dd=pd.DataFrame(reduced_cr)
dd.head()

dd.shape

col_list=cr_dummy.columns
col_list

pd.DataFrame(pc_final.components_.T, columns=['PC_' +str(i) for i in range(10)],index=col_list)

# varianza explicada por cada componente- 
pd.Series(pc_final.explained_variance_ratio_,index=['PC_'+ str(i) for i in range(10)])

"""Determinando el numero óptimo de cluster"""

from sklearn.cluster import KMeans
km_4=KMeans(n_clusters=5,random_state=123)
km_4.fit(reduced_cr)

km_4.labels_
pd.Series(km_4.labels_).value_counts()

cluster_range = range( 1, 21 )
cluster_errors = []

for num_clusters in cluster_range:
    clusters = KMeans( num_clusters )
    clusters.fit( reduced_cr )
    cluster_errors.append( clusters.inertia_ )

clusters_df = pd.DataFrame( { "num_clusters":cluster_range, "cluster_errors": cluster_errors } )

clusters_df[0:21]

# Commented out IPython magic to ensure Python compatibility.
# allow plots to appear in the notebook
# %matplotlib inline
import matplotlib.pyplot as plt
plt.figure(figsize=(5,5))
plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = "o" )

from sklearn import metrics
# calculate SC for K=3 through K=12
k_range = range(2, 21)
scores = []
for k in k_range:
    km = KMeans(n_clusters=k, random_state=1)
    km.fit(reduced_cr)
    scores.append(metrics.silhouette_score(reduced_cr, km.labels_))

scores

# plot the results
plt.plot(k_range, scores)
plt.xlabel('Clúster')
plt.ylabel('Coeficiente de Silueta')
plt.grid(True)

df_pair_plot=pd.DataFrame(reduced_cr,columns=['PC_' +str(i) for i in range(10)])
df_pair_plot['Cluster']=km_5.labels_ #Add cluster column in the data frame
df_pair_plot.head()

#pairwise relationship of components on the data
sns.pairplot(df_pair_plot,hue='Cluster', palette= 'cubehelix', diag_kind='kde',size=1.85)

"""Muestra que los dos primeros componentes pueden identificar clústeres

Ahora que hemos terminado aquí con el componente principal, tenemos que traer nuestro marco de datos original y fusionaremos el clúster con ellos.

"""

col_kpi=['PURCHASES_TRX','Monthly_avg_purchase','Monthly_cash_advance','limit_usage','CASH_ADVANCE_TRX',
         'payment_minpay','both_oneoff_installment','istallment','one_off','none','CREDIT_LIMIT']
cr_pre.describe()

km_5=KMeans(n_clusters=5,random_state=123)
km_5=km_5.fit(reduced_cr)
km_5.labels_

pd.Series(km_5.labels_).value_counts()

plt.figure(figsize=(7,7))
plt.scatter(reduced_cr[:,0],reduced_cr[:,1],c=km_5.labels_,cmap='Dark2',alpha=0.5)
plt.xlabel('')
plt.ylabel('')
plt.title('Clúster existentes')

cluster_df_5=pd.concat([cre_original,pd.Series(km_5.labels_,name='Cluster_5')],axis=1)
cluster_df_5.head(5)

cluster_df_5.groupby('Cluster_5').apply(lambda x: x['BALANCE'].mean())

cluster_df_5.groupby('Cluster_5').apply(lambda x: x['CASH_ADVANCE'].mean())

s1=cluster_df_5.groupby('Cluster_5').apply(lambda x: x['CREDIT_LIMIT'].mean())
print (s1)

s1=cluster_df_5.groupby('Cluster_5').apply(lambda x: x['Cluster_5'].value_counts())
print (s1)

print ("Cluster-5"),'\n'
per_5=pd.Series((s1.values.astype('float')/ cluster_df_5.shape[0])*100,name='Percentage')
print (pd.concat([pd.Series(s1.values,name='Size'),per_5],axis=1))

s1=cluster_df_5.groupby('Cluster_5').apply(lambda x: np.mean(x['limit_usage']))
print (s1)

s1=cluster_df_5.groupby('Cluster_5').apply(lambda x: np.mean(x['PRC_FULL_PAYMENT']))
print (s1)

s1=cluster_df_5.groupby('Cluster_5').apply(lambda x: np.mean(x['Monthly_avg_purchase']))
print (s1)

"""tenemos un grupo de clientes (grupo 2) que tiene las compras promedio más altas, pero hay un grupo 4 que también tiene el mayor anticipo de efectivo y el segundo comportamiento de compra más alto, pero su tipo de compras es el mismo.

Cluster 0 y Cluster 4 se comportan de manera similar en términos de Credit_limit y tienen transacciones en efectivo en el lado superior

normalizar la data y utilizar el método de euclides,
después de normalizar la data ver la relación entre variables, en base a la relación de las variables haré el cluster.
¿porqué normalizar la data?
-https://colab.research.google.com/github/jumafernandez/BDM/blob/master/Guias/Guia_Clustering.ipynb#scrollTo=TrY-Krjv_Df1-

Generalmente, cuando trabajemos con algoritmos de _clustering_, dado que son algoritmos basados en distancias, va a ser fundamental escalar los datos para prescindir de las unidades de medida de las diferentes features.
"""